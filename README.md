#<b> Toxic-Comment-Classification-NLP</b>


We built a multi-headed model that’s capable of detecting different types of toxicity like
threats, obscenity, insults, and identity-based hate better than Perspective’s current models. 

<b> Data </b>

We have a large number of Wikipedia Comments which have been labeled by human raters for toxic behaviour. The types of toxicity are
:
* Toxic

* Severe_toxic

* Obscene

* Threat

* Insult

* Identity_hate

You must create a model which predicts a probability of each type of toxicity for each comment.

<b> Target </b>


LSTM is used to build the above model with some data preprocessing techniques. 


Test Accuracy Acieved: 95.95%
